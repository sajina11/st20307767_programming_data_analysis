{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (3.9.3)\n",
      "Collecting folium\n",
      "  Using cached folium-0.19.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.0-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (11.0.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.29.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-18.1.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting branca>=0.6.0 (from folium)\n",
      "  Using cached branca-0.8.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jinja2>=2.9 (from folium)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting xyzservices (from folium)\n",
      "  Using cached xyzservices-2024.9.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.18.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.9->folium)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached charset_normalizer-3.4.0-cp313-cp313-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.22.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
      "Using cached folium-0.19.2-py2.py3-none-any.whl (110 kB)\n",
      "Using cached scikit_learn-1.6.0-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached branca-0.8.0-py3-none-any.whl (25 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached protobuf-5.29.1-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached pyarrow-18.1.0-cp313-cp313-win_amd64.whl (25.1 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached scipy-1.14.1-cp313-cp313-win_amd64.whl (44.5 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached narwhals-1.18.4-py3-none-any.whl (251 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.22.3-cp313-cp313-win_amd64.whl (235 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: xyzservices, watchdog, urllib3, typing-extensions, toml, threadpoolctl, tenacity, smmap, scipy, rpds-py, pyarrow, protobuf, narwhals, mdurl, MarkupSafe, joblib, idna, click, charset-normalizer, certifi, cachetools, blinker, attrs, scikit-learn, requests, referencing, markdown-it-py, jinja2, gitdb, rich, pydeck, jsonschema-specifications, gitpython, branca, jsonschema, folium, altair, streamlit\n",
      "Successfully installed MarkupSafe-3.0.2 altair-5.5.0 attrs-24.3.0 blinker-1.9.0 branca-0.8.0 cachetools-5.5.0 certifi-2024.12.14 charset-normalizer-3.4.0 click-8.1.7 folium-0.19.2 gitdb-4.0.11 gitpython-3.1.43 idna-3.10 jinja2-3.1.4 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.18.4 protobuf-5.29.1 pyarrow-18.1.0 pydeck-0.9.1 referencing-0.35.1 requests-2.32.3 rich-13.9.4 rpds-py-0.22.3 scikit-learn-1.6.0 scipy-1.14.1 smmap-5.0.1 streamlit-1.41.1 tenacity-9.0.0 threadpoolctl-3.5.0 toml-0.10.2 typing-extensions-4.12.2 urllib3-2.2.3 watchdog-6.0.0 xyzservices-2024.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit seaborn matplotlib folium scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "       r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Aotizhongxin_20130301-20170228.csv',\n",
    "    r\"C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Changping_20130301-20170228.csv\",\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Dingling_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Dongsi_20130301-20170228.csv',\n",
    "   \n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Guanyuan_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Gucheng_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Huairou_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Nongzhanguan_20130301-20170228.csv',\n",
    "    \n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Shunyi_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Tiantan_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Wanliu_20130301-20170228.csv',\n",
    "    r'C:\\Users\\Dell\\OneDrive\\Desktop\\programmingdata\\Dataset Folder for your Final Assessment-20241125\\PRSA_Data_Wanshouxigong_20130301-20170228.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(paths):\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df['Station'] = path.split(\"\\\\\")[-1].split(\"_\")[2]  # Extract station name from filename\n",
    "        dfs.append(df)\n",
    "    merged_data = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # EDA Preprocessing steps\n",
    "    # 1. Handle missing values\n",
    "    merged_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "    \n",
    "    # 2. Remove duplicate entries\n",
    "    merged_data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # 3. Feature engineering (example: create a new 'Month' column from the 'year' column)\n",
    "    merged_data['year'] = pd.to_datetime(merged_data['year'], errors='coerce')\n",
    "    merged_data['Month'] = merged_data['year'].dt.month  # Extract month from 'year' as a new feature\n",
    "    \n",
    "    # Drop rows where 'year' is NaT after conversion\n",
    "    merged_data.dropna(subset=['year'], inplace=True)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_15036\\2945014250.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n"
     ]
    }
   ],
   "source": [
    "data = load_and_merge_data(data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Beijing Air Quality Data Analysis\")\n",
    "    st.sidebar.title(\"Options\")\n",
    "    show_data = st.sidebar.checkbox(\"Data Handling\")\n",
    "    summary_stats = st.sidebar.checkbox(\"Exploratory Data Analysis (EDA)\")\n",
    "    model_building = st.sidebar.checkbox(\"Machine Learning Model Building\")\n",
    "    model_evaluation = st.sidebar.checkbox(\"Model Evaluation\")\n",
    "\n",
    "    # Show basic insights into the dataset\n",
    "    st.subheader(\"Dataset Insights\")\n",
    "    rows, columns = data.shape\n",
    "    st.write(f\"The dataset contains **{rows} rows** and **{columns} columns**.\")\n",
    "    \n",
    "    st.write(\"The columns in the dataset are:\")\n",
    "    st.write(data.columns.tolist())\n",
    "\n",
    "    # Show data types and check for missing values\n",
    "    st.write(\"\\n**Data Types of Each Column:**\")\n",
    "    st.write(data.dtypes)\n",
    "    missing_values = data.isnull().sum()\n",
    "    st.write(\"\\n**Missing Values:**\")\n",
    "    st.write(missing_values)\n",
    "\n",
    "    # Show raw data\n",
    "    if show_data:\n",
    "        st.subheader(\"Raw Merged Data\")\n",
    "        st.dataframe(data.head())\n",
    "\n",
    "    # Show summary statistics\n",
    "    if summary_stats:\n",
    "        st.subheader(\"Summary Statistics\")\n",
    "        st.write(data.describe())  # Displays summary statistics (mean, std, min, max, etc.)\n",
    "\n",
    "        # Display graphs for each column\n",
    "        st.subheader(\"Visualizations of Features\")\n",
    "\n",
    "        # List of columns to graph (select numeric columns for visualization)\n",
    "        numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        for column in numeric_columns:\n",
    "            st.subheader(f\"Visualization for {column}\")\n",
    "            # Plot the distribution of each column\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(data[column], kde=True, color='skyblue', bins=30)\n",
    "            st.pyplot(plt)\n",
    "\n",
    "            # Box Plot for detecting outliers\n",
    "            st.subheader(f\"Box Plot for {column}\")\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(data[column], color='lightgreen')\n",
    "            st.pyplot(plt)\n",
    "\n",
    "    # Map for highest pollution levels\n",
    "    st.subheader(\"Map: Highest Pollution Locations\")\n",
    "    \n",
    "    # Check if latitude and longitude columns exist in the dataset\n",
    "    if 'latitude' in data.columns and 'longitude' in data.columns:\n",
    "        # Find the station with the highest pollution (based on PM2.5 or another metric)\n",
    "        highest_pollution_station = data.loc[data['PM2.5'].idxmax()]\n",
    "\n",
    "        # Create a map centered on Beijing\n",
    "        m = folium.Map(location=[39.9042, 116.4074], zoom_start=10)\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add markers for the station with the highest pollution\n",
    "        folium.Marker(\n",
    "            location=[highest_pollution_station['latitude'], highest_pollution_station['longitude']],\n",
    "            popup=f\"Station: {highest_pollution_station['Station']}<br>PM2.5: {highest_pollution_station['PM2.5']}\",\n",
    "            icon=folium.Icon(color='red')\n",
    "        ).add_to(marker_cluster)\n",
    "        st.write(\"The station with the highest pollution is displayed on the map.\")\n",
    "        st.components.v1.html(m._repr_html_(), height=500)\n",
    "    else:\n",
    "        st.write(\"Latitude and Longitude data are missing. Please check the dataset.\")\n",
    "\n",
    "    # Model Building\n",
    "    if model_building:\n",
    "        st.subheader(\"Machine Learning Model Building\")\n",
    "\n",
    "        # Selecting target and features\n",
    "        target_column = st.selectbox(\"Select Target Variable:\", data.select_dtypes(include=['float64', 'int64']).columns.tolist())\n",
    "        features = data.drop(columns=['year', 'Station', target_column])  # Drop 'year' and 'Station' columns for features\n",
    "        target = data[target_column]\n",
    "\n",
    "        # Encoding categorical features (if any)\n",
    "        features = pd.get_dummies(features, drop_first=True)\n",
    "\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Choose model\n",
    "        model_type = st.selectbox(\"Select Model:\", [\"Linear Regression\", \"Random Forest\", \"Other\"])\n",
    "        if model_type == \"Linear Regression\":\n",
    "            model = LinearRegression()\n",
    "        elif model_type == \"Random Forest\":\n",
    "            model = RandomForestRegressor(random_state=42)\n",
    "        else:\n",
    "            st.warning(\"Other models can be implemented here.\")\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluation\n",
    "        if model_evaluation:\n",
    "            st.subheader(\"Model Evaluation\")\n",
    "            st.write(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "            st.write(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "            st.write(\"R-Squared:\", r2_score(y_test, y_pred))\n",
    "\n",
    "            # Hyperparameter tuning (optional)\n",
    "            if model_type == \"Random Forest\":\n",
    "                st.subheader(\"Hyperparameter Tuning\")\n",
    "                param_grid = {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [10, 20],\n",
    "                }\n",
    "                grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3)\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                st.write(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
